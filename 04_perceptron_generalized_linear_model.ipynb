{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LECTURE 04: Perceptron and Generalized Linear Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boilerplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch # type: ignore\n",
    "import numpy as np\n",
    "import numpy.typing as npt\n",
    "import matplotlib.pyplot as plt # type: ignore\n",
    "from typing import Callable\n",
    "\n",
    "from sklearn.datasets import make_classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GML with hypothesis as input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def hypothesisMatrix(X: np.ndarray,\n",
    "                     theta: np.ndarray,\n",
    "                     hypothesis: Callable[[np.ndarray | float], np.ndarray | float]) -> np.ndarray:\n",
    "\n",
    "    \"\"\"Computes hypothesis values for each sample in the input matrix.\n",
    "\n",
    "    Applies the given hypothesis function to the linear combination (dot product)\n",
    "    of input features and parameters for each sample in the dataset.\n",
    "\n",
    "    Args:\n",
    "        X: Input feature matrix of shape (m_samples, n_features) where each row\n",
    "           represents one sample/observation.\n",
    "        theta: Parameter vector of shape (n_features,) containing the weights\n",
    "               for each feature.\n",
    "        hypothesis: Function that maps the linear predictor (η = θ.T dot X.T) to the\n",
    "                   hypothesis/prediction space. Should accept either array or\n",
    "                   float input and return corresponding output.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: Array of hypothesis values for each sample, with shape\n",
    "                      (n_samples,). The i-th element corresponds to the\n",
    "                      hypothesis value for the i-th sample in X.\n",
    "\n",
    "    Example:\n",
    "        >>> X = np.array([[1, 2], [1, 3]])\n",
    "        >>> theta = np.array([0.5, 1.0])\n",
    "        >>> sigmoid = lambda eta: 1/(1+np.exp(-eta))\n",
    "        >>> hypothesisMatrix(X, theta, sigmoid)\n",
    "        array([0.88079708, 0.95257413])  # sigmoid(2.5), sigmoid(3.5)\n",
    "    \"\"\"\n",
    "\n",
    "    etaArr =  theta.T @ X.T\n",
    "    return np.array([hypothesis(eta) for eta in etaArr])\n",
    "\n",
    "\n",
    "\n",
    "def updateParameterMatrix(Y: np.ndarray,\n",
    "                          X: np.ndarray,\n",
    "                          theta: np.ndarray,\n",
    "                          hypothesisMatrixFunction: Callable[[np.ndarray, np.ndarray], np.ndarray | float],\n",
    "                          learningRate: float) -> np.ndarray:\n",
    "    \n",
    "    \"\"\" Compute theta prime using GLM update rule\n",
    "\n",
    "    Update parameter matrix using GLM update rule with the given hypothesis function\n",
    "\n",
    "    Args:\n",
    "        Y: Output vector where each element is output of one sample\n",
    "        X: Input matrix of shape (m_sample, n_features) where each row\n",
    "           represents the input of one sample.\n",
    "        theta: parameter vector containing weight of each feature\n",
    "        hypothesis: function that maps parameter matrix θ and input matrix X to a\n",
    "                    hypothesis output\n",
    "    \n",
    "    Returns:\n",
    "        numpy.ndarray: Array of updated parameter vector theta(θ') of size (n_features,)\n",
    "    \"\"\"\n",
    "    m = Y.size\n",
    "    prediction = hypothesisMatrix(X= X, theta= theta, hypothesis= hypothesisMatrixFunction)\n",
    "    error = prediction - Y\n",
    "    gradient = X.T @ error.T\n",
    "\n",
    "    return theta - learningRate / m * gradient\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypothesis functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hypothesisMatrixSoftmax(eta: np.ndarray) -> np.ndarray:\n",
    "\n",
    "    \"\"\" Computes hypothesis matrix for softmax function with given eta (η = θ.T dot X.T)\n",
    "    \n",
    "    Args:\n",
    "        eta: η = θ.T dot X.T, input for hypothesis function, shape (k_classes, m_samples)\n",
    "    \n",
    "    Returns:\n",
    "        numpy.ndarray: Softmax hypothesis matrix, shape (k_classes, m_samples)\n",
    "    \"\"\"\n",
    "\n",
    "    etaExp              = np.exp(eta)\n",
    "\n",
    "    # Denominator: sum over classes (axis=1), shape (m,)\n",
    "    denominator = np.sum(etaExp, axis=0, keepdims=True)  # shape: (1, m)\n",
    "\n",
    "    # Element-wise division, broadcasted over each row\n",
    "    hypothesis = etaExp / denominator  # shape: (m, k)\n",
    "    \n",
    "    return hypothesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, Loss: 1.1039\n",
      "Iteration 100, Loss: 1.1039\n",
      "Iteration 200, Loss: 1.1039\n",
      "Iteration 300, Loss: 1.1039\n",
      "Iteration 400, Loss: 1.1039\n",
      "Iteration 500, Loss: 1.1039\n",
      "Iteration 600, Loss: 1.1039\n",
      "Iteration 700, Loss: 1.1039\n",
      "Iteration 800, Loss: 1.1039\n",
      "Iteration 900, Loss: 1.1039\n"
     ]
    }
   ],
   "source": [
    "def test_glm_softmax_classification():\n",
    "    # Set random seed for reproducibility\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Generate synthetic data with 3 classes and some overlap\n",
    "    n_samples = 500\n",
    "    n_features = 2\n",
    "    n_classes = 3\n",
    "    \n",
    "    # Generate data with some overlap between classes\n",
    "    X, y = make_classification(n_samples=n_samples, \n",
    "                              n_features=n_features,\n",
    "                              n_informative=2,\n",
    "                              n_redundant=0,\n",
    "                              n_classes=n_classes,\n",
    "                              n_clusters_per_class=1,\n",
    "                              class_sep=1.0,\n",
    "                              random_state=42)\n",
    "    \n",
    "    # Add bias term to X\n",
    "    X_with_bias = np.c_[np.ones((n_samples, 1)), X]  # shape (500, 3)\n",
    "    \n",
    "    # Initialize parameters (theta) - shape (n_classes, n_features + 1)\n",
    "    theta = np.random.randn(n_classes, n_features + 1) * 0.01  # shape (3, 3)\n",
    "    \n",
    "    # Learning parameters\n",
    "    learning_rate = 0.1\n",
    "    n_iterations = 1000\n",
    "    \n",
    "    # Create one-hot encoded y matrix - shape (n_samples, n_classes)\n",
    "    y_onehot = np.eye(n_classes)[y]  # shape (500, 3)\n",
    "    \n",
    "    # Training loop\n",
    "    for iteration in range(n_iterations):\n",
    "        # Compute logits (scores)\n",
    "        logits = X_with_bias @ theta.T  # shape (500, 3)\n",
    "        \n",
    "        # Compute softmax probabilities\n",
    "        exp_logits = np.exp(logits - np.max(logits, axis=1, keepdims=True))\n",
    "        probs = exp_logits / np.sum(exp_logits, axis=1, keepdims=True)\n",
    "        \n",
    "        # Compute loss (cross-entropy)\n",
    "        loss = -np.mean(np.sum(y_onehot * np.log(probs + 1e-15), axis=1))\n",
    "        \n",
    "        if iteration % 100 == 0:\n",
    "            print(f\"Iteration {iteration}, Loss: {loss:.4f}\")\n",
    "        \n",
    "        # Compute gradient\n",
    "        error = probs - y_onehot  # shape (500, 3)\n",
    "        gradient = error.T @ X_with_bias  # shape (3, 3)\n",
    "        \n",
    "        # Update parameters\n",
    "\n",
    "test_glm_softmax_classification()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
