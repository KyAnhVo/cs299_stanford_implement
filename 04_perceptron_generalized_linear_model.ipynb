{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LECTURE 04: Perceptron and Generalized Linear Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boilerplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch # type: ignore\n",
    "import numpy as np\n",
    "import numpy.typing as npt\n",
    "import matplotlib.pyplot as plt # type: ignore\n",
    "from typing import Callable\n",
    "\n",
    "from sklearn.datasets import make_classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GML with hypothesis as input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def updateParameterMatrix(Y: np.ndarray,\n",
    "                          X: np.ndarray,\n",
    "                          theta: np.ndarray,\n",
    "                          hypothesisMatrixFunction: Callable[[np.ndarray, np.ndarray], np.ndarray | float],\n",
    "                          learningRate: float) -> np.ndarray:\n",
    "    \n",
    "    \"\"\" Compute theta prime using GLM update rule\n",
    "\n",
    "    Update parameter matrix using GLM update rule with the given hypothesis function\n",
    "\n",
    "    Args:\n",
    "        Y: Output vector where each element is output of one sample\n",
    "        X: Input matrix of shape (m_sample, n_features) where each row\n",
    "           represents the input of one sample.\n",
    "        theta: parameter vector containing weight of each feature\n",
    "        hypothesis: function that maps parameter matrix θ and input matrix X to a\n",
    "                    hypothesis output\n",
    "    \n",
    "    Returns:\n",
    "        numpy.ndarray: Array of updated parameter vector theta(θ') of size (n_features,)\n",
    "    \"\"\"\n",
    "    m = Y.shape[0]\n",
    "    eta = theta.T @ X.T\n",
    "    prediction = hypothesisMatrixFunction(eta)\n",
    "    error = prediction - Y\n",
    "    gradient = X.T @ error.T\n",
    "\n",
    "    return theta - learningRate / m * gradient\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hypothesisMatrixSoftmax(eta: np.ndarray) -> np.ndarray:\n",
    "\n",
    "    \"\"\" Computes hypothesis matrix for softmax function with given eta (η = θ.T dot X.T)\n",
    "    \n",
    "    Args:\n",
    "        eta: η = θ.T dot X.T, input for hypothesis function, shape (k_classes, m_samples)\n",
    "    \n",
    "    Returns:\n",
    "        numpy.ndarray: Softmax hypothesis matrix, shape (k_classes, m_samples)\n",
    "    \"\"\"\n",
    "\n",
    "    etaExp              = np.exp(eta)\n",
    "\n",
    "    # Denominator: sum over classes (axis=1), shape (m,)\n",
    "    denominator = np.sum(etaExp, axis=0, keepdims=True)  # shape: (1, m)\n",
    "\n",
    "    # Element-wise division, broadcasted over each row\n",
    "    hypothesis = etaExp / denominator  # shape: (m, k)\n",
    "    \n",
    "    return hypothesis\n",
    "\n",
    "def softmaxRegression(X:np.ndarray, Y: np.ndarray, alpha: float, epochs: int) -> np.ndarray:\n",
    "    n, k = X.shape[1], Y.shape[0]\n",
    "    theta = np.zeros((n, k))\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        theta = updateParameterMatrix(Y, X, theta, hypothesisMatrixSoftmax, alpha)\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate synthetic dataset\n",
    "def generate_synthetic_data(n_samples=100, n_features=5, n_classes=3, random_state=42):\n",
    "    \"\"\"Generate synthetic classification data with one-hot encoded labels\"\"\"\n",
    "    X, y_integer = make_classification(\n",
    "        n_samples=n_samples, \n",
    "        n_features=n_features, \n",
    "        n_classes=n_classes, \n",
    "        n_informative=3, \n",
    "        random_state=random_state\n",
    "    )\n",
    "    \n",
    "    # Convert integer labels to one-hot encoding\n",
    "    Y = np.zeros((n_samples, n_classes))\n",
    "    for i in range(n_samples):\n",
    "        Y[i, y_integer[i]] = 1\n",
    "    \n",
    "    return X, Y\n",
    "\n",
    "# Test the softmax regression implementation\n",
    "def test_softmax_regression():\n",
    "    # Generate synthetic data\n",
    "    X, Y = generate_synthetic_data(n_samples=200, n_features=5, n_classes=3)\n",
    "    \n",
    "    # Normalize features for better convergence\n",
    "    X = (X - X.mean(axis=0)) / X.std(axis=0)\n",
    "    \n",
    "    # Hyperparameters\n",
    "    alpha = 0.1  # Learning rate\n",
    "    epochs = 1000  # Number of iterations\n",
    "    \n",
    "    # Run softmax regression\n",
    "    theta = softmaxRegression(X, Y, alpha, epochs)\n",
    "    \n",
    "    # Print the output (theta values)\n",
    "    print(f\"Trained theta values: \\n{theta}\")\n",
    "    \n",
    "    # Make predictions\n",
    "    eta = X @ theta\n",
    "    predictions = hypothesisMatrixSoftmax(eta)\n",
    "    predicted_classes = np.argmax(predictions, axis=1)\n",
    "    actual_classes = np.argmax(Y, axis=1)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = np.mean(predicted_classes == actual_classes)\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    \n",
    "    # Visualize decision boundaries if features are 2D\n",
    "    if X.shape[1] >= 2:\n",
    "        plot_decision_boundary(X[:, :2], actual_classes, theta[:2, :], predictions)\n",
    "    \n",
    "    return theta, accuracy\n",
    "\n",
    "def plot_decision_boundary(X, y, theta, predictions):\n",
    "    \"\"\"Plot the decision boundary for 2D data\"\"\"\n",
    "    # Set figure size\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    # Create a mesh grid\n",
    "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    h = 0.02  # Step size in the mesh\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "    \n",
    "    # Plot the decision boundary\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.title('Decision Boundary')\n",
    "    \n",
    "    # Plot data points\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, edgecolors='k', cmap=plt.cm.Paired)\n",
    "    plt.xlabel('Feature 1')\n",
    "    plt.ylabel('Feature 2')\n",
    "    \n",
    "    # Plot the confidence of predictions\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.title('Prediction Confidence')\n",
    "    \n",
    "    # Get the confidence of the predictions (max probability)\n",
    "    confidence = np.max(predictions, axis=1)\n",
    "    \n",
    "    # Plot data points with color based on confidence\n",
    "    scatter = plt.scatter(X[:, 0], X[:, 1], c=confidence, cmap='viridis', \n",
    "                         edgecolors='k', alpha=0.7)\n",
    "    plt.colorbar(scatter, label='Confidence')\n",
    "    plt.xlabel('Feature 1')\n",
    "    plt.ylabel('Feature 2')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "theta, accuracy = test_softmax_regression()\n",
    "print(f\"Final theta shape: {theta.shape}\")\n",
    "print(f\"Final accuracy: {accuracy:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
