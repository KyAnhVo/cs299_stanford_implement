{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LECTURE 04: Perceptron and Generalized Linear Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boilerplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch # type: ignore\n",
    "import numpy as np\n",
    "import numpy.typing as npt\n",
    "import matplotlib.pyplot as plt # type: ignore\n",
    "from typing import Callable\n",
    "\n",
    "from sklearn.datasets import make_classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GML with hypothesis as input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def updateParameterMatrix(Y: np.ndarray,\n",
    "                          X: np.ndarray,\n",
    "                          theta: np.ndarray,\n",
    "                          hypothesisMatrixFunction: Callable[[np.ndarray, np.ndarray], np.ndarray | float],\n",
    "                          learningRate: float) -> np.ndarray:\n",
    "    \n",
    "    \"\"\" Compute theta prime using GLM update rule\n",
    "\n",
    "    Update parameter matrix using GLM update rule with the given hypothesis function\n",
    "\n",
    "    Args:\n",
    "        Y: Output vector where each element is output of one sample\n",
    "        X: Input matrix of shape (m_sample, n_features) where each row\n",
    "           represents the input of one sample.\n",
    "        theta: parameter vector containing weight of each feature\n",
    "        hypothesis: function that maps parameter matrix θ and input matrix X to a\n",
    "                    hypothesis output\n",
    "    \n",
    "    Returns:\n",
    "        numpy.ndarray: Array of updated parameter vector theta(θ') of size (n_features,)\n",
    "    \"\"\"\n",
    "    m = Y.shape[0]\n",
    "    eta = theta.T @ X.T\n",
    "    prediction = hypothesisMatrixFunction(eta)\n",
    "    error = prediction - Y\n",
    "    gradient = X.T @ error.T\n",
    "\n",
    "    return theta - learningRate / m * gradient\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hypothesisMatrixSoftmax(eta: np.ndarray) -> np.ndarray:\n",
    "\n",
    "    \"\"\" Computes hypothesis matrix for softmax function with given eta (η = θ.T dot X.T)\n",
    "    \n",
    "    Args:\n",
    "        eta: η = θ.T dot X.T, input for hypothesis function, shape (k_classes, m_samples)\n",
    "    \n",
    "    Returns:\n",
    "        numpy.ndarray: Softmax hypothesis matrix, shape (k_classes, m_samples)\n",
    "    \"\"\"\n",
    "\n",
    "    etaExp              = np.exp(eta)\n",
    "\n",
    "    # Denominator: sum over classes (axis=1), shape (m,)\n",
    "    denominator = np.sum(etaExp, axis=0, keepdims=True)  # shape: (1, m)\n",
    "\n",
    "    # Element-wise division, broadcasted over each row\n",
    "    hypothesis = etaExp / denominator  # shape: (m, k)\n",
    "    \n",
    "    return hypothesis\n",
    "\n",
    "def softmaxRegression(X:np.ndarray, Y: np.ndarray, alpha: float, epochs: int) -> np.ndarray:\n",
    "    n, k = X.shape[1], Y.shape[0]\n",
    "    theta = np.zeros((n, k))\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        theta = updateParameterMatrix(Y, X, theta, hypothesisMatrixSoftmax, alpha)\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (200,200) (3,200) ",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[160]\u001b[39m\u001b[32m, line 35\u001b[39m\n\u001b[32m     32\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m error < \u001b[32m0.1\u001b[39m, \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mExpected error to be below 0.1, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00merror\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     34\u001b[39m \u001b[38;5;66;03m# Run the test\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m \u001b[43mtest_softmax_regression\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[160]\u001b[39m\u001b[32m, line 17\u001b[39m, in \u001b[36mtest_softmax_regression\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     14\u001b[39m epochs = \u001b[32m1000\u001b[39m  \u001b[38;5;66;03m# Number of iterations\u001b[39;00m\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# Run softmax regression\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m theta = \u001b[43msoftmaxRegression\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# Print the output (theta values)\u001b[39;00m\n\u001b[32m     20\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTrained theta values: \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mtheta\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[159]\u001b[39m\u001b[32m, line 27\u001b[39m, in \u001b[36msoftmaxRegression\u001b[39m\u001b[34m(X, Y, alpha, epochs)\u001b[39m\n\u001b[32m     24\u001b[39m theta = np.zeros((n, k))\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m     theta = \u001b[43mupdateParameterMatrix\u001b[49m\u001b[43m(\u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtheta\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhypothesisMatrixSoftmax\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m theta\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[158]\u001b[39m\u001b[32m, line 25\u001b[39m, in \u001b[36mupdateParameterMatrix\u001b[39m\u001b[34m(Y, X, theta, hypothesisMatrixFunction, learningRate)\u001b[39m\n\u001b[32m     23\u001b[39m eta = theta.T @ X.T\n\u001b[32m     24\u001b[39m prediction = hypothesisMatrixFunction(eta)\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m error = \u001b[43mprediction\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m.\u001b[49m\u001b[43mT\u001b[49m\n\u001b[32m     26\u001b[39m gradient = X.T @ error.T\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m theta - learningRate / m * gradient\n",
      "\u001b[31mValueError\u001b[39m: operands could not be broadcast together with shapes (200,200) (3,200) "
     ]
    }
   ],
   "source": [
    "# Function to generate synthetic dataset\n",
    "def generate_synthetic_data(n_samples: int = 100, n_features: int = 5, n_classes: int = 3):\n",
    "    X, Y = make_classification(n_samples=n_samples, n_features=n_features, n_classes=n_classes, n_informative=3, random_state=42)\n",
    "    Y = np.eye(n_classes)[Y]  # One-hot encoding of the labels\n",
    "    return X, Y\n",
    "\n",
    "# Test softmax regression\n",
    "def test_softmax_regression():\n",
    "    # Generate synthetic data\n",
    "    X, Y = generate_synthetic_data(n_samples=200, n_features=5, n_classes=3)\n",
    "\n",
    "    # Hyperparameters for testing\n",
    "    alpha = 0.01  # Learning rate\n",
    "    epochs = 1000  # Number of iterations\n",
    "\n",
    "    # Run softmax regression\n",
    "    theta = softmaxRegression(X, Y, alpha, epochs)\n",
    "\n",
    "    # Print the output (theta values)\n",
    "    print(f\"Trained theta values: \\n{theta}\")\n",
    "\n",
    "    # Test: Check if the theta matrix is of the expected shape (n_features, n_classes)\n",
    "    assert theta.shape == (X.shape[1], Y.shape[1]), f\"Expected shape {(X.shape[1], Y.shape[1])}, but got {theta.shape}\"\n",
    "\n",
    "    # Optionally: Check if the model has converged or if there's any improvement in the error\n",
    "    eta = X @ theta  # compute eta = X * theta\n",
    "    prediction = hypothesisMatrixSoftmax(eta.T)  # hypothesis matrix\n",
    "    error = np.mean(np.argmax(prediction, axis=0) != np.argmax(Y, axis=1))  # Classification error\n",
    "    print(f\"Final classification error: {error}\")\n",
    "\n",
    "    # Assert that the error is below a threshold\n",
    "    assert error < 0.1, f\"Expected error to be below 0.1, but got {error}\"\n",
    "\n",
    "# Run the test\n",
    "test_softmax_regression()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
